# "Bias Variance" exposition

Daniela Witten says the tradeoff is related to the equation

$$\text{Exp. Pred. Error }=\text{ Irreducible Error + Bias}^2+ \text{Var}$$

# From Ben Recht slides

**Given**: i.i.d. sample $$S=\left\{z_1, \ldots, z_n\right\}$$ from distribution $D$\
**Goal**: Find a good predictor function $f$\
Population risk (test error): $$R[f]=\mathbb{E}_z \operatorname{loss}(f ; z)$$ Unknown!\
Empirical risk (training error): $$R_s[f]=\frac{1}{n} \sum_{i=1}^n \operatorname{loss}\left(f ; z_i\right)$$ Minimize using SGD!\
Generalization error: $R[f]-R_s[f]$

How much empirical risk underestimates population risk

We can compute $R_s \ldots$:

"Fundamental theorem of ML:"

$$R[f]=\left(R[f]-R_S[f]\right)+R_S[f]$$

population generalization training - small training error implies risk Â± generalization error - zero training error does not imply overfitting $$\begin{aligned}
R[f] &=\left(R[f]-R\left[f_{\mathcal{H}}\right]\right) \\
&+\left(R\left[f_{\mathcal{H}}\right]-R\left[f_{\star}\right]\right) \\
&+R\left[f_{\star}\right]
\end{aligned}$$
